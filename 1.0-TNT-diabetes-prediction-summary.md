# Diabetes Prediction Model - Project Summary

**Author:** TNT  
**Version:** 1.0  
**Date:** August 5, 2025  
**Script:** `scripts/1.0-TNT-diabetes-prediction.py`

## Project Overview

This project implements a machine learning pipeline to predict diabetes using the Pima Indians Diabetes dataset. The model uses patient health metrics to classify whether an individual has diabetes or not.

## Dataset Information

- **Source:** `data/diabetes.csv`
- **Total Samples:** 768 patients
- **Features:** 8 health-related attributes
- **Target:** Binary classification (0 = No Diabetes, 1 = Diabetes)
- **Class Distribution:** 
  - No Diabetes: 500 patients (65.1%)
  - Diabetes: 268 patients (34.9%)

### Features Used:
1. **Pregnancies** - Number of times pregnant
2. **Glucose** - Plasma glucose concentration
3. **BloodPressure** - Diastolic blood pressure (mm Hg)
4. **SkinThickness** - Triceps skin fold thickness (mm)
5. **Insulin** - 2-Hour serum insulin (mu U/ml)
6. **BMI** - Body mass index (weight in kg/(height in m)^2)
7. **DiabetesPedigreeFunction** - Diabetes pedigree function
8. **Age** - Age in years

## Data Preprocessing

- **Missing Values:** Handled zero values in key columns (Glucose, BloodPressure, SkinThickness, Insulin, BMI) by replacing with median values
- **Feature Scaling:** Applied StandardScaler for Logistic Regression
- **Train/Test Split:** 80/20 split with stratification (614 training, 154 testing samples)

## Models Trained

### 1. Random Forest Classifier ⭐ **Best Model**
- **Accuracy:** 77.92%
- **ROC-AUC:** 0.8192
- **Precision (No Diabetes):** 80%
- **Recall (No Diabetes):** 88%
- **Precision (Diabetes):** 73%
- **Recall (Diabetes):** 59%

### 2. Logistic Regression
- **Accuracy:** 70.78%
- **ROC-AUC:** 0.8130
- **Precision (No Diabetes):** 75%
- **Recall (No Diabetes):** 82%
- **Precision (Diabetes):** 60%
- **Recall (Diabetes):** 50%

## Model Performance Analysis

The **Random Forest** model outperformed Logistic Regression with:
- Higher overall accuracy (77.92% vs 70.78%)
- Better precision for diabetes prediction (73% vs 60%)
- Better recall for diabetes prediction (59% vs 50%)
- Slightly higher ROC-AUC score (0.8192 vs 0.8130)

## Files Generated

### Models (saved in `output/` directory):
- `1.0-TNT-random_forest_model_20250805_205151.pkl` - Trained Random Forest model
- `1.0-TNT-logistic_regression_model_20250805_205151.pkl` - Trained Logistic Regression model
- `1.0-TNT-scaler_20250805_205151.pkl` - Feature scaler for preprocessing

### Results (saved in `results/` directory):
- `1.0-TNT-model_results_20250805_205151.txt` - Detailed performance metrics
- `1.0-TNT-model_comparison_20250805_205151.png` - Model comparison visualization
- `1.0-TNT-confusion_matrices_20250805_205151.png` - Confusion matrices for both models

## Key Insights

1. **Model Selection:** Random Forest performed better, likely due to its ability to handle non-linear relationships and feature interactions
2. **Class Imbalance:** The dataset has more non-diabetic cases, which affects model performance on the minority class
3. **Feature Importance:** The Random Forest model can capture complex patterns in health metrics
4. **ROC-AUC Scores:** Both models show good discriminative ability (>0.8), indicating they can effectively distinguish between diabetic and non-diabetic patients

## Usage Instructions

### To Load and Use the Trained Model:
```python
import joblib
import pandas as pd

# Load the best model and scaler
model = joblib.load('output/1.0-TNT-random_forest_model_20250805_205151.pkl')
scaler = joblib.load('output/1.0-TNT-scaler_20250805_205151.pkl')

# Example prediction (replace with actual patient data)
patient_data = [[6, 148, 72, 35, 0, 33.6, 0.627, 50]]  # Example features
prediction = model.predict(patient_data)
probability = model.predict_proba(patient_data)

print(f"Prediction: {'Diabetes' if prediction[0] == 1 else 'No Diabetes'}")
print(f"Probability: {probability[0][1]:.3f}")
```

### To Retrain the Model:
```bash
python3 scripts/1.0-TNT-diabetes-prediction.py
```

## Future Improvements

1. **Feature Engineering:** Create new features or polynomial combinations
2. **Hyperparameter Tuning:** Use GridSearchCV or RandomizedSearchCV
3. **Advanced Models:** Try XGBoost, SVM, or Neural Networks
4. **Cross-Validation:** Implement k-fold cross-validation for more robust evaluation
5. **Feature Selection:** Use techniques like RFE or feature importance analysis
6. **Handle Class Imbalance:** Apply SMOTE or other resampling techniques

## Conclusion

The diabetes prediction model successfully achieved 77.92% accuracy with the Random Forest classifier. The model demonstrates good performance in identifying both diabetic and non-diabetic patients, with an ROC-AUC score of 0.8192, indicating strong discriminative ability. The saved models and preprocessing components are ready for deployment or further analysis.
